{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home07/bham/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/n/home07/bham/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Inspecting Helper Functions\n",
    "\n",
    "import zstandard as zstd\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "import os\n",
    "import tarfile\n",
    "import gzip\n",
    "import io\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def jsonl_beginning(filename, num_bytes=1000):\n",
    "    \"\"\"\n",
    "    Prints the beginning characters of a jsonl zst compressed file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    decompressed = dctx.decompress(data)\n",
    "    print(decompressed[:num_bytes])\n",
    "    return\n",
    "\n",
    "def extract_tar_file(tar_path, extract_to=\"./extracted\"):\n",
    "    \"\"\"Extract the tar file to a given directory.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(extract_to, exist_ok=True)\n",
    "        with tarfile.open(tar_path, 'r') as tar:\n",
    "            print(f\"Extracting files to: {extract_to}\")\n",
    "            tar.extractall(path=extract_to)\n",
    "            print(\"Extraction complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting the tar file: {e}\")\n",
    "\n",
    "\n",
    "def tar_beginning(filename, num_bytes=1000):\n",
    "    \"\"\"\n",
    "    Prints the beginning characters or bytes of a file inside a tar archive.\n",
    "    Handles text and gzip-compressed files.\n",
    "    \"\"\"\n",
    "    with tarfile.open(filename, \"r\") as tar:\n",
    "        member = tar.getmembers()[0]\n",
    "        print(f\"Inspecting: {member.name}\")\n",
    "        file_obj = tar.extractfile(member) \n",
    "        \n",
    "        if file_obj:\n",
    "            data = file_obj.read() \n",
    "            \n",
    "            # Handle gzip-compressed content\n",
    "            if data[:2] == b'\\x1f\\x8b':  # GZIP magic number\n",
    "                print(\"Detected gzip-compressed content. Decompressing...\")\n",
    "                with gzip.GzipFile(fileobj=io.BytesIO(data)) as gzip_file:\n",
    "                    decompressed_data = gzip_file.read()\n",
    "                try:\n",
    "                    token_ids = ast.literal_eval(decompressed_data.decode('utf-8', errors='replace'))\n",
    "                    print(token_ids[:num_bytes])\n",
    "\n",
    "                    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "                    print(\"Decoded Text:\")\n",
    "                    print(decoded_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error decoding decompressed content: {e}\")\n",
    "            else:\n",
    "                # Attempt to decode as UTF-8 for text files\n",
    "                try:\n",
    "                    text_data = data.decode('utf-8', errors='replace')\n",
    "                    print(text_data[:num_bytes])\n",
    "                    token_ids = tokenizer.encode(text_data)\n",
    "                    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "                    print(\"Decoded text:\")\n",
    "                    print(decoded_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error decoding file content: {e}\")\n",
    "        else:\n",
    "            print(\"Could not extract the file content.\")\n",
    "\n",
    "def memmap_beginning(filename, num_tokens=1000):\n",
    "    \"\"\"\n",
    "    Prints the beginning characters of a memmapped file.\n",
    "    \"\"\"\n",
    "    # Memory-map the .npy file\n",
    "    data = np.memmap(filename, dtype=\"uint16\", mode='r')\n",
    "\n",
    "    print(\"Data dtype:\", data.dtype)\n",
    "    print(data[:num_tokens])\n",
    "    return list(data[:num_tokens])\n",
    "\n",
    "def csv_beginning(filename, num_lines=5):\n",
    "    \"\"\"\n",
    "    Prints the first few lines of a CSV file compressed by gz (.csv.gz).\n",
    "    \"\"\"\n",
    "    with gzip.open(filename, 'rt') as file:\n",
    "        for i in range(num_lines):\n",
    "            print(file.readline(), end='')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Padded Downstream Dataset, memmap format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/n/netscratch/sham_lab/Everyone/dclm/color_filter/data/memmap/2048_core-task-trainsets-v3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m \u001b[38;5;66;03m# number of tokens per file to inspect\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Loop over all .npy files found in memmap_folder\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemmap_folder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      7\u001b[0m         memmap_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(memmap_folder, file)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/n/netscratch/sham_lab/Everyone/dclm/color_filter/data/memmap/2048_core-task-trainsets-v3'"
     ]
    }
   ],
   "source": [
    "memmap_folder = \"/n/netscratch/sham_lab/Everyone/dclm/color_filter/data/memmap/2048_core-task-trainsets-v3\"\n",
    "num_tokens = 2048 # number of tokens per file to inspect\n",
    "\n",
    "# Loop over all .npy files found in memmap_folder\n",
    "for file in os.listdir(memmap_folder):\n",
    "    if file.endswith(\".npy\"):\n",
    "        memmap_file = os.path.join(memmap_folder, file)\n",
    "        print(f\"Inspecting file: {file}\")\n",
    "        tokens = memmap_beginning(memmap_file, num_tokens)\n",
    "        print(\"Len tokens:\", len(tokens))\n",
    "        print(tokenizer.decode(tokens, skip_special_tokens=True))\n",
    "        print(\"\\n\\n----------------------------------------\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
